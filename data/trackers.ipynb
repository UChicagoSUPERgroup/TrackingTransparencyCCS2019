{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to better.fyi data\n",
    "# available at https://github.com/UChicagoSUPERgroup/better-content\n",
    "better_path = '../../better-content'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers for markdown parsing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import mistune\n",
    "def target_links(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.find_all('a')\n",
    "    for link in links:\n",
    "        link['target'] = '_blank'\n",
    "    return str(soup)\n",
    "\n",
    "def markdown(md):\n",
    "    return target_links(mistune.markdown(md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process better data\n",
    "\n",
    "better_data = {}\n",
    "trackers = os.listdir(better_path + '/trackers')\n",
    "for tracker in trackers:\n",
    "    filename = better_path + '/trackers/' + tracker + '/index.md'\n",
    "    f = open(filename, \"r\")\n",
    "    lines=f.readlines()\n",
    "    name = lines[0].split('**')[1]\n",
    "    description = markdown(lines[2][2:])\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"<!-- prevalence -->\\n\":\n",
    "            prevalence = markdown(lines[i+1])\n",
    "        if line == \"## Notes\\n\":\n",
    "            notes = markdown(''.join(lines[i+2:]))\n",
    "    better_data[tracker] = {\n",
    "        'name': name,\n",
    "        'description': description,\n",
    "        'prevalence': prevalence,\n",
    "        'notes': notes\n",
    "    }\n",
    "\n",
    "with open('better.json', 'w') as f:\n",
    "  json.dump(better_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disconnect list processing\n",
    "\n",
    "# originally based off of https://github.com/duckduckgo/duckduckgo-privacy-extension/blob/418e30d36e1c24e27930acb534caeb3ffc81c6a8/scripts/importers/companyList.js\n",
    "# (with a few subsequent rewrites and translation to python)\n",
    "\n",
    "domainEntityMap = {}\n",
    "companyData = {}\n",
    "\n",
    "with open('disconnect.json') as disconnect_json:\n",
    "    disconnect_list = json.load(disconnect_json)\n",
    "    \n",
    "    with open('better.json') as better_json:\n",
    "        better_data = json.load(better_json)\n",
    "\n",
    "        for type in disconnect_list['categories']:\n",
    "            for entry in disconnect_list['categories'][type]:\n",
    "                (name, info) = list(entry.items())[0]\n",
    "                (site, domains) = list(info.items())[0]\n",
    "\n",
    "                cleanedSite = urlparse(site).hostname\n",
    "                if not cleanedSite:\n",
    "                    cleanedSite = site[:-1]\n",
    "                cleanedSite = cleanedSite.split('www.')[-1]\n",
    "\n",
    "                data = {\n",
    "                    'site': cleanedSite,\n",
    "                    'domains': domains,\n",
    "                    'type': type\n",
    "                }\n",
    "                if cleanedSite in better_data:\n",
    "                    better = better_data[cleanedSite]\n",
    "                    data['description'] = better['description']\n",
    "                    data['prevalence'] = better['prevalence']\n",
    "                    data['notes'] = better['notes']\n",
    "\n",
    "                companyData[name] = data\n",
    "\n",
    "                for domain in domains:\n",
    "                    domainEntityMap[domain] = name\n",
    "\n",
    "\n",
    "# facebook, twitter, and google are classified under the \"disconnect\" category for legacy reasons\n",
    "# so we recategorize them under the correct categories\n",
    "companyData['Facebook']['type'] = 'Social'\n",
    "companyData['Twitter']['type'] = 'Social'\n",
    "companyData['Google']['type'] = 'Advertising' # what is google?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../src/data/trackers/companyData.json', 'w') as f:\n",
    "  json.dump(companyData, f, ensure_ascii=False, indent=2)\n",
    "with open('../src/data/trackers/domainEntityMap.json', 'w') as f:\n",
    "  json.dump(domainEntityMap, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
